import os
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import logging
import re
from datetime import datetime, timedelta
import shutil


class TourReopenProcessor:
    """
    处理淡季Tour重开的类
    """

    def __init__(self):
        # 设置基础目录和路径
        self.BASE_DIR = os.path.dirname(os.path.abspath(__file__))
        self.INPUT_DIR = os.path.join(self.BASE_DIR, 'Spreadsheets Source')
        self.OUTPUT_DIR = os.path.join(self.BASE_DIR, 'Outputs')
        self.PUBLIC_DIR = r"C:\City Experience\Public Data Base"  # 保持绝对路径

        # 设置日志
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        # 初始化数据存储
        self.tours_open_data = None
        self.summary_stacked_data = None
        self.tour_id_map = {}
        self.low_season_start_date_map = {}  # 新增：存储每个 Tour ID 的 "孰晚" 开始日期

    def _clean_tour_id_column(self, df, column_name='Tour ID'):
        """
        辅助函数：确保 Tour ID 列是字符串类型并移除 '.0' 后缀。
        """
        if column_name in df.columns:
            try:
                # 尝试将可能是浮点数的数字转换为整数，再转字符串
                # 使用 Int64Dtype 处理 NaN 值
                df[column_name] = df[column_name].astype(float).astype(pd.Int64Dtype()).astype(str)
            except (ValueError, TypeError):
                # 如果有非数字Tour ID，则直接转为字符串并清理
                df[column_name] = df[column_name].astype(str)

            df[column_name] = df[column_name].str.replace(r'\.0$', '', regex=True)
            df[column_name] = df[column_name].str.strip()  # 移除可能的空白字符
        return df

    def read_tours_open_data(self, time_agg_level='weekly'):
        """
        步骤1: 读取Tours Open in Low Season数据并更新Overall Growth Rate
        """
        try:
            tours_file = os.path.join(self.INPUT_DIR, 'Tours Open in Low Season.xlsx')
            self.tours_open_data = pd.read_excel(tours_file)
            logging.info(f"成功读取Tours Open数据: {tours_file}")

            self.tours_open_data = self._clean_tour_id_column(self.tours_open_data)
            logging.debug(
                f"Tours Open data 'Tour ID's cleaned. Example: {self.tours_open_data['Tour ID'].head().tolist()}")

            self.tours_open_data['Open Start Date'] = pd.to_datetime(self.tours_open_data['Open Start Date'])
            self.tours_open_data['Open End Date'] = pd.to_datetime(self.tours_open_data['Open End Date'])

            actuals_file = os.path.join(
                self.OUTPUT_DIR,
                time_agg_level.capitalize(),
                f'actuals_{time_agg_level}_horizontal.csv'
            )

            if os.path.exists(actuals_file):
                actuals_data = pd.read_csv(actuals_file)
                actuals_data = self._clean_tour_id_column(actuals_data)

                total_growth_row = actuals_data[actuals_data['Tour ID'] == 'total']
                if not total_growth_row.empty:
                    overall_growth_rate = total_growth_row['Sigmoid_Growth_Rate'].iloc[0]
                    self.tours_open_data['Overall Growth Rate'] = overall_growth_rate
                    logging.info(f"成功更新总体增长率 (Overall Growth Rate): {overall_growth_rate:.4f}")
                else:
                    logging.warning(
                        f"在 {actuals_file} 中未找到 Tour ID 'total'，无法更新 Overall Growth Rate。设置为 0。")
                    self.tours_open_data['Overall Growth Rate'] = 0.0
            else:
                logging.warning(f"未找到 actuals 文件: {actuals_file}，无法更新 Overall Growth Rate。设置为 0。")
                self.tours_open_data['Overall Growth Rate'] = 0.0

            self.tours_open_data.to_excel(tours_file, index=False)
            logging.info(f"已使用更新的 'Overall Growth Rate' 覆盖 '{tours_file}'。")

            self.tour_id_map = dict(zip(
                self.tours_open_data['Tour ID'],
                self.tours_open_data['Event Name']
            ))

            return self.tours_open_data

        except Exception as e:
            logging.error(f"读取 Tours Open 数据时出错: {e}", exc_info=True)
            raise

    def read_and_filter_summary_stacked(self, time_agg_level='weekly'):
        """
        步骤2: 读取Summary stacked数据，确定“孰晚”开始日期，并筛选出需要处理的数据。
        """
        try:
            summary_file = os.path.join(
                self.OUTPUT_DIR,
                time_agg_level.capitalize(),
                f'summary_stacked_{time_agg_level}.csv'
            )

            if not os.path.exists(summary_file):
                logging.error(f"Summary stacked 文件未找到: {summary_file}")
                raise FileNotFoundError(f"Summary stacked file not found: {summary_file}")

            self.summary_stacked_data = pd.read_csv(summary_file)
            self.summary_stacked_data = self._clean_tour_id_column(self.summary_stacked_data)
            logging.debug(
                f"Summary stacked data 'Tour ID's cleaned. Example: {self.summary_stacked_data['Tour ID'].head().tolist()}")

            self.summary_stacked_data['Date'] = pd.to_datetime(self.summary_stacked_data['Date'])
            logging.info(f"成功读取Summary stacked数据: {summary_file}")

            # --- 新增逻辑：确定“孰晚”日期 ---
            self.low_season_start_date_map = {}
            for _, tour_row in self.tours_open_data.iterrows():
                tour_id = tour_row['Tour ID']
                open_start_date = tour_row['Open Start Date']

                # 筛选当前 Tour ID 的 Summary Stacked 数据中 Actual 不为空的最新日期
                tour_summary_data = self.summary_stacked_data[self.summary_stacked_data['Tour ID'] == tour_id]
                latest_actual_date = tour_summary_data[tour_summary_data['Actual'].notna()]['Date'].max()

                if pd.isna(latest_actual_date):
                    # 如果没有 Actual 数据，则使用 Open Start Date
                    孰晚日期 = open_start_date
                    logging.debug(
                        f"  Tour ID '{tour_id}': 没有 Actual 数据，使用 Open Start Date: {孰晚日期.strftime('%Y-%m-%d')}")
                else:
                    # 取 Open Start Date 和 latest_actual_date 的孰晚
                    孰晚日期 = max(open_start_date, latest_actual_date)
                    logging.debug(
                        f"  Tour ID '{tour_id}': Open Start Date: {open_start_date.strftime('%Y-%m-%d')}, Latest Actual Date: {latest_actual_date.strftime('%Y-%m-%d')}. '孰晚'日期: {孰晚日期.strftime('%Y-%m-%d')}")

                self.low_season_start_date_map[tour_id] = 孰晚日期
            # --- “孰晚”日期逻辑结束 ---

            # 筛选需要处理的数据
            tours_low_season_before = self._filter_low_season_data()

            return tours_low_season_before

        except Exception as e:
            logging.error(f"读取Summary stacked数据时出错: {e}", exc_info=True)
            raise

    def _filter_low_season_data(self):
        """
        筛选出Tours Open in Low Season中的Tour ID在指定日期范围内的数据，
        使用计算出的“孰晚”开始日期。
        """
        if self.tours_open_data is None or self.summary_stacked_data is None or not self.low_season_start_date_map:
            raise ValueError("需要先读取Tours Open和Summary stacked数据，并计算'孰晚'日期。")

        filtered_data_list = []

        for _, tour_row in self.tours_open_data.iterrows():
            tour_id = tour_row['Tour ID']
            open_end_date = tour_row['Open End Date']

            # 使用“孰晚”日期作为开始日期
            actual_start_date = self.low_season_start_date_map.get(tour_id)
            if actual_start_date is None:
                logging.warning(f"Tour ID '{tour_id}' 没有对应的 '孰晚' 日期，跳过筛选。")
                continue

            # 筛选当前 Tour ID 且日期在 [actual_start_date, open_end_date] 范围内的数据
            mask = (
                    (self.summary_stacked_data['Tour ID'] == tour_id) &
                    (self.summary_stacked_data['Date'] >= actual_start_date) &
                    (self.summary_stacked_data['Date'] <= open_end_date)
            )

            tour_data = self.summary_stacked_data[mask].copy()
            if not tour_data.empty:
                filtered_data_list.append(tour_data)

        if filtered_data_list:
            tours_low_season_before = pd.concat(filtered_data_list, ignore_index=True)
            logging.info(f"根据 '孰晚' 日期筛选出 {len(tours_low_season_before)} 条淡季 Tour 数据。")
            logging.debug(
                f"筛选出的淡季数据示例 (Tour ID, Date): \n{tours_low_season_before[['Tour ID', 'Date']].head()}")
        else:
            logging.warning("未找到符合条件的淡季 Tour 数据 (基于 '孰晚' 日期)。")
            tours_low_season_before = pd.DataFrame(columns=self.summary_stacked_data.columns)  # 返回空 DataFrame 但保持列结构

        return tours_low_season_before

    def read_and_process_pax_historical(self, time_agg_level='weekly', reference_years=None):
        """
        步骤3: 读取Pax历史数据并处理。
        根据传入的时间级别和参考年份，查找同期数据并计算 Growth_Adj_Pred。
        """
        if reference_years is None:
            reference_years = []

        try:
            pax_file = os.path.join(self.OUTPUT_DIR, f'grouped_data_{time_agg_level}_True.csv')

            if not os.path.exists(pax_file):
                logging.error(f"Pax 历史数据文件未找到: {pax_file}")
                raise FileNotFoundError(f"Pax historical data file not found: {pax_file}")

            pax_data = pd.read_csv(pax_file)
            pax_data = self._clean_tour_id_column(pax_data)
            logging.debug(f"Pax historical data 'Tour ID's cleaned. Example: {pax_data['Tour ID'].head().tolist()}")

            date_col = 'WeekStart' if time_agg_level == 'weekly' else 'Date'

            if date_col not in pax_data.columns:
                logging.error(f"Pax 历史数据中缺少日期列: '{date_col}'")
                raise ValueError(f"Missing date column '{date_col}' in pax historical data.")

            pax_data[date_col] = pd.to_datetime(pax_data[date_col])

            logging.info(f"成功读取Pax历史数据: {pax_file}, 共 {len(pax_data)} 条记录")
            logging.info(
                f"数据日期范围: {pax_data[date_col].min().strftime('%Y-%m-%d')} 到 {pax_data[date_col].max().strftime('%Y-%m-%d')}")

            # 处理参考年限数据
            processed_data = self._process_reference_years(pax_data, date_col, reference_years, time_agg_level)

            return processed_data

        except Exception as e:
            logging.error(f"读取 Pax 历史数据时出错: {e}", exc_info=True)
            raise

    def _process_reference_years(self, pax_data, date_col, reference_years, time_agg_level):
        """
        处理参考年限的同期数据。
        对于周级别，根据 ISO 周数进行匹配。
        并且根据“孰晚”日期截取历史数据。
        """
        if self.tours_open_data is None or not self.low_season_start_date_map:
            raise ValueError("需要先读取 Tours Open 数据和计算'孰晚'日期。")

        processed_data_list = []

        if isinstance(reference_years, (int, str)):
            reference_years = [reference_years]

        logging.info(f"开始处理参考年份: {reference_years}")

        if 'Actual Pax' not in pax_data.columns:
            logging.error("Pax 历史数据中缺少 'Actual Pax' 列。")
            raise ValueError("Missing 'Actual Pax' column in pax historical data.")

        for _, tour_row in self.tours_open_data.iterrows():
            tour_id = tour_row['Tour ID']
            open_end_date = tour_row['Open End Date']  # 目标年份的开放结束日期 (datetime)
            overall_growth = tour_row['Overall Growth Rate']
            additional_growth = tour_row['Expected Additional Growth Rate']

            # 使用“孰晚”日期作为此 Tour 的实际起始日期
            actual_start_date = self.low_season_start_date_map.get(tour_id)
            if actual_start_date is None:
                logging.warning(f"Tour ID '{tour_id}' 没有对应的 '孰晚' 日期，跳过其历史数据查找。")
                continue

            logging.info(
                f"处理 Tour ID: '{tour_id}', 实际起始日期: {actual_start_date.strftime('%Y-%m-%d')} 到 目标结束日期: {open_end_date.strftime('%Y-%m-%d')}")

            tour_pax_data = pax_data[pax_data['Tour ID'] == tour_id].copy()

            if tour_pax_data.empty:
                logging.warning(f"  在 Pax 历史数据中未找到 Tour ID '{tour_id}' 的记录，跳过该 Tour 的历史数据查找。")
                continue

            for ref_year in reference_years:
                ref_year = int(ref_year)
                logging.info(f"  查找 Tour ID '{tour_id}' 在 {ref_year} 年的参考数据。")

                ref_data_for_year = pd.DataFrame()

                if time_agg_level == 'weekly':
                    if 'week_num' not in tour_pax_data.columns or 'year' not in tour_pax_data.columns:
                        tour_pax_data['week_num'] = tour_pax_data[date_col].dt.isocalendar().week
                        tour_pax_data['year'] = tour_pax_data[date_col].dt.year

                    ref_data_for_year = self._get_weekly_reference_data_with_late_start(
                        tour_pax_data, tour_id, actual_start_date, open_end_date, ref_year, date_col
                    )
                else:  # daily
                    ref_data_for_year = self._get_daily_reference_data_with_late_start(
                        tour_pax_data, tour_id, actual_start_date, open_end_date, ref_year, date_col
                    )

                if not ref_data_for_year.empty:
                    logging.info(f"    找到 {len(ref_data_for_year)} 条 Tour ID '{tour_id}' 在 {ref_year} 年的数据。")

                    # 计算 Growth_Adj_Pred 并四舍五入为整数
                    ref_data_for_year['Growth_Adj_Pred'] = (
                            ref_data_for_year['Actual Pax'] *
                            (1 + overall_growth) *
                            (1 + additional_growth)
                    ).round().astype(int)  # 四舍五入并转为整数

                    # 将历史数据的日期调整到目标年份 (即 open_start_date.year)
                    target_year = actual_start_date.year  # 使用当前处理 Tour 的目标年份
                    if time_agg_level == 'weekly':
                        # 对于周级别，需要将参考年的 WeekStart 调整到目标年份的同周日期
                        # 注意：这种直接替换年份的方法在极端情况下（比如跨年周）可能不精确
                        # 更稳健的方法是基于 WeekNum 来构建目标年份的日期，但这里为了简化先直接替换年份
                        ref_data_for_year['Date'] = ref_data_for_year[date_col].apply(
                            lambda x: x.replace(year=target_year))
                    else:  # daily
                        ref_data_for_year['Date'] = ref_data_for_year[date_col].apply(
                            lambda x: x.replace(year=target_year))

                    # 只保留用于合并的必要列
                    ref_data_for_year = ref_data_for_year[['Tour ID', 'Date', 'Actual Pax', 'Growth_Adj_Pred']]
                    processed_data_list.append(ref_data_for_year)
                    ref_data_for_year.to_csv('ref_data_for_year.csv')
                else:
                    logging.warning(f"    未找到 Tour ID '{tour_id}' 在 {ref_year} 年的参考数据 (基于 '孰晚' 日期)。")

        if processed_data_list:
            result = pd.concat(processed_data_list, ignore_index=True)
            logging.info(f"总共处理了 {len(result)} 条参考年份数据。")
            logging.debug(
                f"处理后的历史数据示例 (Tour ID, Date, Growth_Adj_Pred): \n{result[['Tour ID', 'Date', 'Growth_Adj_Pred']].head()}")
            return result
        else:
            logging.warning("未找到任何参考年份数据。返回空 DataFrame。")
            return pd.DataFrame(columns=['Tour ID', 'Date', 'Actual Pax', 'Growth_Adj_Pred'])

    def _get_weekly_reference_data_with_late_start(self, tour_pax_data_filtered, tour_id, actual_start_date,
                                                   open_end_date, ref_year, date_col):
        """
        获取周级别的参考年份数据，考虑“孰晚”开始日期。
        """
        # 获取目标周期（即 open_start_date -> open_end_date，但实际从 actual_start_date 开始）的周数
        # 这里的周数仍然是基于目标年份的开放日期计算
        start_week = actual_start_date.isocalendar().week
        end_week = open_end_date.isocalendar().week

        logging.info(
            f"      目标年份 ({actual_start_date.year}) 的实际开放周数范围: 第 {start_week} 周到第 {end_week} 周。")

        # 筛选参考年份的数据，根据年份和周数
        mask = (
                (tour_pax_data_filtered['year'] == ref_year) &
                (tour_pax_data_filtered['week_num'] >= start_week) &
                (tour_pax_data_filtered['week_num'] <= end_week)
        )

        result = tour_pax_data_filtered[mask].copy()

        # 调试信息
        if not tour_pax_data_filtered.empty:
            available_years = tour_pax_data_filtered['year'].unique()
            available_weeks_in_ref_year = tour_pax_data_filtered[tour_pax_data_filtered['year'] == ref_year][
                'week_num'].unique()
            logging.debug(f"      Tour '{tour_id}' 在 Pax 历史数据中可用年份: {sorted(available_years)}")
            logging.debug(f"      Tour '{tour_id}' 在 {ref_year} 年可用周数: {sorted(available_weeks_in_ref_year)}")
        else:
            logging.debug(f"      Tour '{tour_id}' 在 Pax 历史数据中没有记录。")

        return result

    def _get_daily_reference_data_with_late_start(self, tour_pax_data_filtered, tour_id, actual_start_date,
                                                  open_end_date, ref_year, date_col):
        """
        获取日级别的参考年份数据，考虑“孰晚”开始日期。
        """
        # 计算参考年份的同期日期，基于“孰晚”开始日期
        try:
            ref_start = actual_start_date.replace(year=ref_year)
            ref_end = open_end_date.replace(year=ref_year)
        except ValueError:
            def safe_replace_year(dt, target_year):
                try:
                    return dt.replace(year=target_year)
                except ValueError:
                    return dt.replace(year=target_year, day=28)

            ref_start = safe_replace_year(actual_start_date, ref_year)
            ref_end = safe_replace_year(open_end_date, ref_year)

        logging.info(
            f"      查找目标日期范围 (参考年 {ref_year}, 基于 '孰晚' 日期): {ref_start.strftime('%Y-%m-%d')} 到 {ref_end.strftime('%Y-%m-%d')}")

        mask = (
                (tour_pax_data_filtered[date_col] >= ref_start) &
                (tour_pax_data_filtered[date_col] <= ref_end)
        )

        result = tour_pax_data_filtered[mask].copy()

        # 调试信息
        if not tour_pax_data_filtered.empty:
            available_years = tour_pax_data_filtered[date_col].dt.year.unique()
            available_dates_in_ref_year = tour_pax_data_filtered[
                (tour_pax_data_filtered[date_col].dt.year == ref_year)
            ][date_col].dt.date.unique()
            logging.debug(f"      Tour '{tour_id}' 在 Pax 历史数据中可用年份: {sorted(available_years)}")
            logging.debug(f"      Tour '{tour_id}' 在 {ref_year} 年可用日期数量: {len(available_dates_in_ref_year)}。")
        else:
            logging.debug(f"      Tour '{tour_id}' 在 Pax 历史数据中没有记录。")

        return result

    def process_low_season_data(self, tours_low_season_before_process, pax_historical_data_after_process):
        """
        步骤4: 处理Tours low season数据。
        现在，这个函数的主要作用是确保 `tours_low_season_before_process` 中的记录
        被 `pax_historical_data_after_process` 中的 `Growth_Adj_Pred` 值覆盖。
        我们不再进行额外的连接，而是直接利用 `pax_historical_data_after_process`
        作为“处理后的淡季数据”，因为它已经包含了调整后的预测。
        """
        # 实际上，pax_historical_data_after_process 已经是我们想要合并到 summary_stacked_data 中的数据
        # 它包含了针对淡季 Tour ID 和日期计算出的 Growth_Adj_Pred

        # 确保返回的数据只包含 Tour ID, Date, Growth_Adj_Pred
        # 同时保留 tours_low_season_before_process 中的其他列，并用新计算的 Growth_Adj_Pred 覆盖
        if tours_low_season_before_process.empty or pax_historical_data_after_process.empty:
            logging.warning("低季数据或历史处理数据为空。返回原始低季数据，并尝试用 Pred 填充 Growth_Adj_Pred。")
            if 'Growth_Adj_Pred' not in tours_low_season_before_process.columns:
                tours_low_season_before_process['Growth_Adj_Pred'] = np.nan
            tours_low_season_before_process['Growth_Adj_Pred'] = tours_low_season_before_process[
                'Growth_Adj_Pred'].fillna(tours_low_season_before_process['Pred'])
            return tours_low_season_before_process

        # 创建一个副本，以便在上面更新 Growth_Adj_Pred
        tours_low_season_after_process = tours_low_season_before_process.copy()

        # 确保 Growth_Adj_Pred 列存在
        if 'Growth_Adj_Pred' not in tours_low_season_after_process.columns:
            tours_low_season_after_process['Growth_Adj_Pred'] = np.nan

        # 使用 DataFrame.update 来根据 'Tour ID' 和 'Date' 更新 Growth_Adj_Pred
        # 需要确保两者的索引对齐或使用相同的合并键作为索引
        merge_keys = ['Tour ID', 'Date']

        indexed_low_season = tours_low_season_after_process.set_index(merge_keys)
        indexed_pax_historical = pax_historical_data_after_process.set_index(merge_keys)

        indexed_low_season.update(indexed_pax_historical[['Growth_Adj_Pred']], overwrite=True)

        tours_low_season_after_process = indexed_low_season.reset_index()

        # 对于那些在 pax_historical_data_after_process 中没有对应记录的淡季数据，
        # 其 Growth_Adj_Pred 仍为 NaN。此时用其 Pred 值填充。
        tours_low_season_after_process['Growth_Adj_Pred'] = tours_low_season_after_process['Growth_Adj_Pred'].fillna(
            tours_low_season_after_process['Pred'])

        logging.info(f"成功处理低季数据，得到 {len(tours_low_season_after_process)} 条记录。")
        logging.debug(
            f"处理后的低季数据示例 (Tour ID, Date, Pred, Growth_Adj_Pred): \n{tours_low_season_after_process[['Tour ID', 'Date', 'Pred', 'Growth_Adj_Pred']].head()}")
        return tours_low_season_after_process

    def merge_back_to_summary(self, tours_low_season_after_process, time_agg_level='weekly'):
        """
        步骤5: 将处理后的数据合并回 Summary stacked，并保存到文件。
        """
        try:
            if self.summary_stacked_data is None:
                raise ValueError("需要先读取Summary stacked数据。")

            summary_after_process = self.summary_stacked_data.copy()

            if 'Growth_Adj_Pred' not in summary_after_process.columns:
                summary_after_process['Growth_Adj_Pred'] = summary_after_process['Pred']

            if not tours_low_season_after_process.empty:
                merge_keys = ['Tour ID', 'Date']

                summary_indexed = summary_after_process.set_index(merge_keys)
                low_season_indexed = tours_low_season_after_process.set_index(merge_keys)

                # 使用 update 方法来更新 Growth_Adj_Pred 列
                summary_indexed.update(low_season_indexed[['Growth_Adj_Pred']], overwrite=True)

                summary_after_process = summary_indexed.reset_index()
            else:
                logging.warning(
                    "没有处理后的淡季数据可用于合并，Summary stacked 数据将保持其 Growth_Adj_Pred 默认值或现有值。")

            summary_after_process['Growth_Adj_Pred'] = summary_after_process['Growth_Adj_Pred'].fillna(
                summary_after_process['Pred'])

            output_file_name = f'summary_stacked_{time_agg_level}.csv'
            original_file_path = os.path.join(
                self.OUTPUT_DIR,
                time_agg_level.capitalize(),
                output_file_name
            )
            public_file_path = os.path.join(
                self.PUBLIC_DIR,
                output_file_name
            )
            os.makedirs(os.path.dirname(public_file_path), exist_ok=True)

            try:
                summary_after_process.to_csv(original_file_path, index=False)
                logging.info(f"成功更新并保存 Summary stacked 数据到: {original_file_path}")
            except Exception as e:
                logging.error(f"保存 Summary stacked 数据到 {original_file_path} 失败: {e}", exc_info=True)

            try:
                summary_after_process.to_csv(public_file_path, index=False)
                logging.info(f"成功更新并保存 Summary stacked 数据到公共目录: {public_file_path}")
            except Exception as e:
                logging.error(f"保存 Summary stacked 数据到 {public_file_path} 失败: {e}", exc_info=True)

            logging.debug(
                f"合并后的 Summary stacked 数据示例 (Tour ID, Date, Pred, Growth_Adj_Pred): \n{summary_after_process[['Tour ID', 'Date', 'Pred', 'Growth_Adj_Pred']].head()}")
            return summary_after_process

        except Exception as e:
            logging.error(f"合并数据时出错: {e}", exc_info=True)
            raise

    def plot_reopen_predictions(self, summary_after_process, time_agg_level='weekly'):
        """
        步骤6: 绘制重开Tour的预测图表。
        """
        try:
            if self.tours_open_data is None:
                raise ValueError("需要先读取Tours Open数据。")

            plot_dir = os.path.join(self.OUTPUT_DIR, time_agg_level.capitalize(), 'Reopen_Plots')
            os.makedirs(plot_dir, exist_ok=True)

            reopen_tour_ids = self.tours_open_data['Tour ID'].tolist()

            for tour_id in reopen_tour_ids:
                logging.info(f"正在为 Tour ID '{tour_id}' 绘制图表...")

                tour_data = summary_after_process[summary_after_process['Tour ID'] == tour_id].copy()

                if tour_data.empty:
                    logging.warning(f"未找到 Tour ID '{tour_id}' 的数据，跳过绘图。")
                    continue

                tour_data['Date'] = pd.to_datetime(tour_data['Date'])
                tour_data = tour_data.sort_values(by='Date')

                dates = tour_data['Date']
                pred_values = tour_data['Pred']
                adj_pred_values = tour_data['Growth_Adj_Pred']
                actual_values = tour_data['Actual']

                plt.figure(figsize=(12, 6))
                plt.plot(dates, pred_values, label='Prediction', marker='x', color='blue', linestyle='--')
                plt.plot(dates, adj_pred_values, label='Growth Adjusted Prediction', marker='^', color='green',
                         linestyle='-.')
                plt.plot(dates, actual_values, label='Actual Pax', marker='o', color='black', linestyle='-')

                plt.legend()
                event_name = self.tour_id_map.get(tour_id, tour_id)
                plt.title(f"{tour_id} - {event_name} Predictions ({time_agg_level.capitalize()})")
                plt.xlabel('Date')
                plt.ylabel('Pax')
                plt.grid(True)

                date_format = '%Y-%m-%d'
                plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(date_format))
                plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
                plt.gcf().autofmt_xdate()

                illegal_chars = r'[\<\>\:"/\\|?*]'
                safe_tour_id = re.sub(illegal_chars, '_', str(tour_id))
                safe_event_name = re.sub(illegal_chars, '_', str(event_name))

                filename = f"Reopen Processed_{safe_tour_id}_{safe_event_name}_{time_agg_level}_predictions.png"
                filepath = os.path.join(plot_dir, filename)

                try:
                    plt.savefig(filepath)
                    logging.info(f"成功保存图表: {filepath}")
                except Exception as e:
                    logging.error(f"保存图表失败 {filepath}: {e}", exc_info=True)

                plt.close()

        except Exception as e:
            logging.error(f"绘制图表时出错: {e}", exc_info=True)
            raise

    def process_tour_reopen(self, time_agg_level='weekly', reference_years=None):
        """
        完整的Tour重开处理流程。
        """
        if reference_years is None:
            reference_years = []

        try:
            logging.info("=========== 开始处理淡季 Tour 重开流程 ===========")
            logging.info(f"配置: 时间级别='{time_agg_level}', 参考年份={reference_years}")

            # 步骤1: 读取Tours Open数据
            logging.info("\n--- 步骤 1: 读取 Tours Open 数据 ---")
            self.read_tours_open_data(time_agg_level)

            # 步骤2: 读取并筛选Summary stacked数据 (在此步骤中计算“孰晚”日期)
            logging.info("\n--- 步骤 2: 读取并筛选 Summary stacked 数据 (并确定 '孰晚' 开始日期) ---")
            tours_low_season_before = self.read_and_filter_summary_stacked(time_agg_level)

            # 步骤3: 读取并处理Pax历史数据 (使用“孰晚”日期进行截取)
            logging.info("\n--- 步骤 3: 读取并处理 Pax 历史数据 (使用 '孰晚' 日期进行截取) ---")
            pax_historical_after = self.read_and_process_pax_historical(time_agg_level, reference_years)

            # 步骤4: 处理低季数据 (将历史调整后的预测合并到低季预测中)
            logging.info("\n--- 步骤 4: 处理低季数据 (将历史数据中的 Growth_Adj_Pred 应用到筛选出的低季数据) ---")
            tours_low_season_after = self.process_low_season_data(
                tours_low_season_before, pax_historical_after
            )

            # 步骤5: 合并回Summary stacked
            logging.info("\n--- 步骤 5: 合并回 Summary stacked ---")
            summary_after_process = self.merge_back_to_summary(tours_low_season_after, time_agg_level)

            # 步骤6: 绘制图表
            logging.info("\n--- 步骤 6: 绘制图表 ---")
            self.plot_reopen_predictions(summary_after_process, time_agg_level)

            logging.info("\n=========== 淡季 Tour 重开处理完成！ ===========")
            return summary_after_process

        except Exception as e:
            logging.error(f"处理过程中出错: {e}", exc_info=True)
            raise


# 运行示例 (请在运行前确保您的 Outputs 和 Public Data Base 文件夹结构存在，或者已通过其他方式生成数据)
if __name__ == "__main__":
    # 配置日志级别以便调试 (可以改为 logging.DEBUG 查看更详细的调试信息)
    logging.getLogger().setLevel(logging.INFO)

    # 创建处理器实例
    processor = TourReopenProcessor()

    # 执行完整处理流程
    # time_agg_level可以是'weekly'或'daily'
    # reference_years现在是具体年份列表，如[2024, 2023]或[2023]
    # 请确保您的'Spreadsheets Source'和'Outputs'目录中存在相应的文件
    try:
        # 假设我们运行 weekly 流程，参考 2023 年的历史数据
        result_df = processor.process_tour_reopen(time_agg_level='weekly', reference_years=[2023])
        print(f"\n最终处理后的 Summary stacked 数据 (周级别) 的前5行:\n{result_df.head()}")

        # 也可以尝试运行 daily 流程
        # processor_daily = TourReopenProcessor() # 创建新的实例，避免状态混淆
        # result_daily_df = processor_daily.process_tour_reopen(time_agg_level='daily', reference_years=[2023])
        # print(f"\n最终处理后的 Summary stacked 数据 (日级别) 的前5行:\n{result_daily_df.head()}")

    except Exception as e:
        print(f"主程序运行失败: {e}")

    print(
        "\n请检查 'Outputs' 文件夹中生成的 CSV 文件和图表，以及 'C:\\City Experience\\Public Data Base' 中的 CSV 文件。")
