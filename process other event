import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import logging
from datetime import datetime
import gc
from dateutil.relativedelta import relativedelta
from aggregate_actuals import ActualsAggregator
import re

BASE_DIR = os.path.dirname(os.path.abspath(__file__))


class PredictionProcessor:
    def __init__(self, start_date='2024-01-01', end_date='2024-12-31', base_dir=BASE_DIR, forecast_start_date=None,
                 sigmoid_k=1, sigmoid_max_rate=0.1886, time_level='weekly'):
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        if self.start_date > self.end_date:
            raise ValueError("start_date 不能晚于 end_date")

        self.base_dir = base_dir
        self.Public_DIR = r"C:\City Experience\Public Data Base"
        
        # 新增：时间级别参数
        valid_time_levels = ['hourly', 'daily', 'weekly', 'monthly']
        if time_level.lower() not in valid_time_levels:
            raise ValueError(f"time_level must be one of {valid_time_levels}")
        self.time_level = time_level.lower()

        if forecast_start_date is None:
            self.forecast_start_date = pd.to_datetime(datetime.now() - relativedelta(months=2)).replace(hour=0,
                                                                                                        minute=0,
                                                                                                        second=0,
                                                                                                        microsecond=0)
        else:
            self.forecast_start_date = pd.to_datetime(forecast_start_date)
        self.all_summary = []
        self.failed_tours = set()
        self.tour_id_map = None
        self.predict_tour_ids = set()
        self.bookings_data = None

        logging.info("正在初始化并运行 ActualsAggregator 以生成实际值 CSV 文件")
        aggregator = ActualsAggregator(base_dir=self.base_dir, start_date=self.start_date, end_date=self.end_date,
                                       sigmoid_k=sigmoid_k, sigmoid_max_rate=sigmoid_max_rate)
        aggregator.process_all_levels()
        logging.info("ActualsAggregator 处理完成，已生成所有时间级别的 actuals CSV 文件")

        if not logging.getLogger().handlers:
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler(os.path.join(self.base_dir, 'process_other_events.log'), encoding='utf-8'),
                    logging.StreamHandler()
                ]
            )
        logging.info(
            f"初始化 PredictionProcessor，时间范围: {self.start_date} 到 {self.end_date}, 预测开始日期: {self.forecast_start_date}, 时间级别: {self.time_level}")

    def load_tour_id_map(self):
        """
        Loads the mapping from Tour ID to Event Name.
        Standardizes Tour IDs (lowercase and remove spaces) to match Tour IDs in processed dataframes.
        """
        tour_id_file = os.path.join(self.Public_DIR, "Tour ID.xlsx")

        if os.path.exists(tour_id_file):
            try:
                tour_df = pd.read_excel(tour_id_file, sheet_name="Tour ID")
                if 'Tour ID' in tour_df.columns and 'Event Name' in tour_df.columns:
                    standardized_tour_ids = tour_df['Tour ID'].astype(str).str.lower().str.strip()
                    self.tour_id_map = dict(zip(standardized_tour_ids, tour_df['Event Name'].astype(str)))
                    logging.info(f"Successfully loaded Tour ID to Event Name map from {tour_id_file}")
                else:
                    logging.warning(f"Sheet 'Tour ID' in {tour_id_file} is missing 'Tour ID' or 'Event Name' columns.")
            except Exception as e:
                logging.error(f"Failed to read {tour_id_file}: {str(e)}")
        else:
            logging.warning(f"Tour ID map file not found at {tour_id_file}. Event Names cannot be mapped.")

    def load_predict_tour_ids(self):
        predict_tour_file = os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID to Predict.xlsx")
        if os.path.exists(predict_tour_file):
            try:
                predict_df = pd.read_excel(predict_tour_file)
                if 'Tour ID' in predict_df.columns:
                    self.predict_tour_ids = set(predict_df['Tour ID'].astype(str).str.lower().str.strip())
                    logging.info(f"成功加载 Tour ID to Predict: {self.predict_tour_ids}")
                else:
                    logging.warning(f"{predict_tour_file} 缺少 'Tour ID' 列")
            except Exception as e:
                logging.error(f"读取 {predict_tour_file} 失败: {str(e)}")
        else:
            logging.warning(f"未找到 {predict_tour_file}")

    def load_bookings_data(self):
        """
        加载并处理bookings data
        """
        bookings_file = os.path.join(self.Public_DIR, "B2B info.xlsx")
        if not os.path.exists(bookings_file):
            logging.error(f"未找到 {bookings_file}")
            return None
        
        try:
            bookings_df = pd.read_excel(bookings_file, sheet_name="Bookings Ventrada")
            
            # 检查必要列
            if 'Travel Date' not in bookings_df.columns or 'PAX' not in bookings_df.columns or 'Tour ID' not in bookings_df.columns:
                logging.error("Bookings Ventrada sheet 缺少必要列")
                return None
            
            # 转换日期
            bookings_df['Travel Date'] = pd.to_datetime(bookings_df['Travel Date'], errors='coerce')
            
            # 获取本周周一日期
            today = pd.Timestamp.now()
            this_monday = today - pd.Timedelta(days=today.weekday())
            this_monday = this_monday.normalize()
            
            # 筛选未来期间的数据（本周周一之后）
            future_bookings = bookings_df[bookings_df['Travel Date'] >= this_monday].copy()
            
            # 标准化Tour ID
            future_bookings['Tour ID'] = future_bookings['Tour ID'].astype(str).str.lower().str.strip()
            
            # 根据时间级别聚合
            if self.time_level == 'weekly':
                # 生成StartOfWeek（周一日期）
                future_bookings['StartOfWeek'] = future_bookings['Travel Date'] - pd.to_timedelta(
                    future_bookings['Travel Date'].dt.dayofweek, unit='d')
                aggregated = future_bookings.groupby(['StartOfWeek', 'Tour ID'])['PAX'].sum().reset_index()
                aggregated.rename(columns={'StartOfWeek': 'Date'}, inplace=True)
            elif self.time_level == 'daily':
                aggregated = future_bookings.groupby(['Travel Date', 'Tour ID'])['PAX'].sum().reset_index()
                aggregated.rename(columns={'Travel Date': 'Date'}, inplace=True)
            elif self.time_level == 'monthly':
                future_bookings['Month'] = future_bookings['Travel Date'].dt.to_period('M').dt.to_timestamp()
                aggregated = future_bookings.groupby(['Month', 'Tour ID'])['PAX'].sum().reset_index()
                aggregated.rename(columns={'Month': 'Date'}, inplace=True)
            else:  # hourly
                # 对于hourly，假设按日聚合（因为Travel Date通常没有小时信息）
                aggregated = future_bookings.groupby(['Travel Date', 'Tour ID'])['PAX'].sum().reset_index()
                aggregated.rename(columns={'Travel Date': 'Date'}, inplace=True)
            
            self.bookings_data = aggregated
            
            # 输出bookings data
            output_dir = os.path.join(self.base_dir, "Outputs", self.time_level)
            os.makedirs(output_dir, exist_ok=True)
            bookings_output_path = os.path.join(output_dir, f"bookings_data_{self.time_level}.csv")
            aggregated.to_csv(bookings_output_path, index=False)
            logging.info(f"Bookings data saved to {bookings_output_path}")
            
            return aggregated
            
        except Exception as e:
            logging.error(f"加载bookings data失败: {str(e)}")
            return None

    def get_agg_folder_map(self):
        outputs_dir = os.path.join(self.base_dir, "Outputs")
        if not os.path.exists(outputs_dir):
            logging.error(f"Outputs 文件夹不存在: {outputs_dir}")
            return {}

        possible_levels = ['hourly', 'daily', 'weekly', 'monthly']
        agg_folder_map = {}
        for folder in os.listdir(outputs_dir):
            folder_path = os.path.join(outputs_dir, folder)
            if os.path.isdir(folder_path) and folder.lower() in possible_levels:
                agg_folder_map[folder.lower()] = folder
        return agg_folder_map

    def check_required_files(self):
        """
        只检查指定时间级别的文件
        """
        agg_folder_map = self.get_agg_folder_map()
        
        # 只检查指定的时间级别
        if self.time_level not in agg_folder_map:
            logging.error(f"未找到时间级别 '{self.time_level}' 对应的文件夹")
            return []
        
        folder = agg_folder_map[self.time_level]
        required_files = [
            os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID Failed to Predict.xlsx"),
            os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID to Predict.xlsx"),
            os.path.join(self.base_dir, "Outputs", folder, f"predictions_{self.time_level}.xlsx"),
            os.path.join(self.base_dir, "Outputs", folder, f"actuals_{self.time_level}_horizontal.csv")
        ]
        
        if all(os.path.exists(file) for file in required_files):
            return [self.time_level]
        else:
            missing = [f for f in required_files if not os.path.exists(f)]
            logging.error(f"缺少必要文件: {missing}")
            return []

    def load_failed_tours(self):
        failed_tours_file = os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID Failed to Predict.xlsx")
        if os.path.exists(failed_tours_file):
            try:
                failed_df = pd.read_excel(failed_tours_file)
                if 'Tour ID' in failed_df.columns:
                    self.failed_tours = set(str(tour).strip().lower() for tour in failed_df['Tour ID'].unique())
                    logging.info(f"检测到建模失败 Tour ID: {self.failed_tours}")
            except Exception as e:
                logging.error(f"读取 {failed_tours_file} 失败: {str(e)}")

    def load_and_pivot_prediction(self, agg_level, folder):
        prediction_file = os.path.join(self.base_dir, "Outputs", folder, f"predictions_{agg_level}.xlsx")
        if not os.path.exists(prediction_file):
            logging.error(f"未找到 {prediction_file}")
            return None

        prediction_df = pd.read_excel(prediction_file, sheet_name=f"{agg_level}_predictions")
        if not {'Date', 'Tour ID', 'Ensemble Prediction'}.issubset(prediction_df.columns):
            logging.error(f"{prediction_file} 缺少必要列")
            return None

        prediction_df['Date'] = pd.to_datetime(prediction_df['Date'], errors='coerce')
        horizontal_prediction = prediction_df.pivot(index='Tour ID', columns='Date', values='Ensemble Prediction')
        horizontal_prediction = horizontal_prediction.reset_index()
        date_format = '%Y-%m-%d %H:%M:%S' if agg_level == 'hourly' else '%Y-%m-%d'
        horizontal_prediction.columns = ['Tour ID'] + [f"{col.strftime(date_format)}_Pred" for col in
                                                       horizontal_prediction.columns[1:]]
        horizontal_prediction['Tour ID'] = horizontal_prediction['Tour ID'].astype(str).str.lower().str.strip()
        return horizontal_prediction

    def filter_successful_tours(self, horizontal_prediction):
        return horizontal_prediction[~horizontal_prediction['Tour ID'].isin(self.failed_tours)]

    def join_actual_and_prediction(self, agg_level, folder, successful_prediction):
        actuals_file = os.path.join(self.base_dir, "Outputs", folder, f"actuals_{agg_level}_horizontal.csv")
        actuals_df = pd.read_csv(actuals_file)
        actuals_df['Tour ID'] = actuals_df['Tour ID'].astype(str).str.lower().str.strip()

        total_pred_row = successful_prediction[successful_prediction['Tour ID'] == 'total']
        if total_pred_row.empty:
            logging.error("未找到 Tour ID = 'total' 的预测数据")
            return None

        joined_df = actuals_df.merge(successful_prediction, on='Tour ID', how='left', suffixes=('_Actual', '_Pred'))
        for col in joined_df.columns:
            if col.endswith('_Pred'):
                failed_mask = joined_df[col].isna()
                total_pred_value = total_pred_row[col].values[0] if not total_pred_row[col].isna().all() else 0
                joined_df.loc[failed_mask, col] = (
                        total_pred_value * joined_df.loc[failed_mask, 'Pax Ratio']).round().astype(int)

        return joined_df

    def generate_summary(self, joined_df, agg_level):
        if self.tour_id_map is None:
            self.load_tour_id_map()

        summary_df = joined_df.copy()
        summary_df['Event Name'] = summary_df['Tour ID'].map(self.tour_id_map).fillna(summary_df['Tour ID'])
        unmapped_tours = summary_df[summary_df['Event Name'] == summary_df['Tour ID']]
        if not unmapped_tours.empty:
            logging.warning(
                f"The following Tour IDs could not be mapped to an Event Name and were filled with their own Tour ID:\n{unmapped_tours[['Tour ID', 'Event Name']].to_string()}")

        date_cols_pred = [col for col in summary_df.columns if col.endswith('_Pred')]
        date_cols_actual = [col for col in summary_df.columns if col.endswith('_Actual')]
        for col in date_cols_pred + date_cols_actual:
            summary_df[col] = summary_df[col].fillna(0).astype(int)

        is_predict_tour = pd.Series(
            summary_df['Tour ID'].isin(self.predict_tour_ids).astype(int),
            index=summary_df.index,
            name='Is_Predict_Tour'
        )
        summary_df = pd.concat([summary_df, is_predict_tour], axis=1)

        summary_df = summary_df.sort_values(by=['Is_Predict_Tour', 'Period Sum'], ascending=[False, False])
        summary_df = summary_df.drop(columns=['Is_Predict_Tour'])

        return summary_df[['Tour ID', 'Event Name'] + date_cols_pred + date_cols_actual +
                          ['YTD_This_Year', 'YTD_Last_Year', 'Growth_Rate', 'Sigmoid_Growth_Rate', 'Period Sum',
                           'Pax Ratio']]

    def generate_forecast(self, source_df, agg_level, output_dir, output_filename):
        date_cols_pred = [col for col in source_df.columns if col.endswith('_Pred')]
        forecast_cols = ['Tour ID', 'Event Name'] + date_cols_pred
        forecast_df = source_df[forecast_cols].copy()

        forecast_df.columns = ['Tour ID', 'Event Name'] + [col.replace('_Pred', '') for col in date_cols_pred]

        date_columns = [col for col in forecast_df.columns if col not in ['Tour ID', 'Event Name']]
        valid_dates = [col for col in date_columns if pd.to_datetime(col) >= self.forecast_start_date]
        forecast_df = forecast_df[['Tour ID', 'Event Name'] + valid_dates]

        forecast_output_path = os.path.join(output_dir, output_filename)
        forecast_df.to_csv(forecast_output_path, index=False)
        logging.info(f"Forecast saved to {forecast_output_path}")
        return forecast_df

    def generate_adjusted_summary(self, summary_df, agg_level, output_dir):
        adjusted_df = summary_df.copy()
        date_cols_pred = [col for col in summary_df.columns if col.endswith('_Pred')]

        for col in date_cols_pred:
            adjusted_df[col] = (adjusted_df[col] * (1 + adjusted_df['Sigmoid_Growth_Rate'].fillna(0))).round().astype(
                int)

        adjusted_output_path = os.path.join(output_dir, f"summary_adjusted_{agg_level}.csv")
        adjusted_df.to_csv(adjusted_output_path, index=False)
        logging.info(f"Growth Rate Adjusted Summary (using Sigmoid_Growth_Rate) saved to {adjusted_output_path}")
        return adjusted_df

    def generate_stacked_summary(self, summary_df, adjusted_df, agg_level, output_dir):
        date_cols_pred = [col for col in summary_df.columns if col.endswith('_Pred')]
        date_cols_actual = [col for col in summary_df.columns if col.endswith('_Actual')]

        pred_melt = pd.melt(
            summary_df,
            id_vars=['Tour ID', 'Event Name'],
            value_vars=date_cols_pred,
            var_name='Date',
            value_name='Pred'
        )
        pred_melt['Date'] = pred_melt['Date'].str.replace('_Pred', '')

        actual_melt = pd.melt(
            summary_df,
            id_vars=['Tour ID', 'Event Name'],
            value_vars=date_cols_actual,
            var_name='Date',
            value_name='Actual'
        )
        actual_melt['Date'] = actual_melt['Date'].str.replace('_Actual', '')

        adj_pred_melt = pd.melt(
            adjusted_df,
            id_vars=['Tour ID', 'Event Name'],
            value_vars=date_cols_pred,
            var_name='Date',
            value_name='Growth_Adj_Pred'
        )
        adj_pred_melt['Date'] = adj_pred_melt['Date'].str.replace('_Pred', '')

        stacked_df = pred_melt.merge(actual_melt, on=['Tour ID', 'Event Name', 'Date'], how='outer')
        stacked_df = stacked_df.merge(adj_pred_melt, on=['Tour ID', 'Event Name', 'Date'], how='outer')
        stacked_df['Date'] = pd.to_datetime(stacked_df['Date'])

        stacked_output_path = os.path.join(output_dir, f"summary_stacked_{agg_level}.csv")
        stacked_df.to_csv(stacked_output_path, index=False)
        logging.info(f"Stacked Summary saved to {stacked_output_path}")
        return stacked_df

    def calculate_low_season_sd(self, stacked_df):
        """
        计算淡季(Q1和Q4)的标准差
        """
        df = stacked_df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        df['Quarter'] = df['Date'].dt.quarter
        
        # 筛选淡季数据（Q1和Q4）
        low_season_df = df[df['Quarter'].isin([1, 4])].copy()
        
        # 计算每个Tour ID的标准差
        sd_df = low_season_df.groupby('Tour ID')['Actual'].std().reset_index()
        sd_df.columns = ['Tour ID', 'Low_Season_SD']
        sd_df['Low_Season_SD'] = sd_df['Low_Season_SD'].fillna(0)
        
        return sd_df

    def get_same_period_last_year(self, date, time_level):
        """
        获取去年同期日期
        """
        if time_level == 'weekly':
            # 对于周级别，找到去年同一周的周一
            last_year_date = date - pd.DateOffset(years=1)
            # 调整到周一
            days_to_monday = last_year_date.weekday()
            last_year_monday = last_year_date - pd.Timedelta(days=days_to_monday)
            return last_year_monday
        else:
            # 对于日级别或其他，直接减一年
            return date - pd.DateOffset(years=1)

    def adjust_stacked_summary(self, stacked_df, output_dir):
        """
        对stacked summary进行调整
        """
        df = stacked_df.copy()
        df['Date'] = pd.to_datetime(df['Date'])
        df['Quarter'] = df['Date'].dt.quarter
        
        # 1. 计算淡季标准差
        sd_df = self.calculate_low_season_sd(df)
        sd_output_path = os.path.join(output_dir, f"low_season_sd_{self.time_level}.csv")
        sd_df.to_csv(sd_output_path, index=False)
        logging.info(f"淡季标准差已保存到 {sd_output_path}")
        
        # 合并标准差到主数据
        df = df.merge(sd_df, on='Tour ID', how='left')
        df['Low_Season_SD'] = df['Low_Season_SD'].fillna(0)
        
        # 2. 合并bookings data
        if self.bookings_data is not None:
            bookings_merge = self.bookings_data.copy()
            bookings_merge['Date'] = pd.to_datetime(bookings_merge['Date'])
            df = df.merge(bookings_merge, on=['Tour ID', 'Date'], how='left', suffixes=('', '_Booked'))
            df['PAX'] = df['PAX'].fillna(0)
        else:
            df['PAX'] = 0
        
        # 3. 应用预订数据调整：如果Growth_Adj_Pred < 已预订的PAX，上调为已预订 + (0.382 * SD)
        mask_below_booking = df['Growth_Adj_Pred'] < df['PAX']
        df.loc[mask_below_booking, 'Growth_Adj_Pred'] = df.loc[mask_below_booking, 'PAX'] + (
            0.382 * df.loc[mask_below_booking, 'Low_Season_SD'])
        
        # 4. 对淡季数据设置上下限
        low_season_mask = df['Quarter'].isin([1, 4])
        
        for idx in df[low_season_mask].index:
            current_date = df.loc[idx, 'Date']
            tour_id = df.loc[idx, 'Tour ID']
            
            # 获取去年同期日期
            last_year_date = self.get_same_period_last_year(current_date, self.time_level)
            
            # 查找去年同期数据
            last_year_data = df[(df['Tour ID'] == tour_id) & (df['Date'] == last_year_date)]
            
            if not last_year_data.empty:
                last_year_actual = last_year_data['Actual'].values[0]
                
                # 计算去年同期淡季的标准差（使用已计算的Low_Season_SD）
                sd = df.loc[idx, 'Low_Season_SD']
                
                # 计算上下限
                lower_bound = last_year_actual * (1 - 0.0618 * sd)
                upper_bound = last_year_actual * (1 + 0.0618 * sd)
                
                # 应用上下限
                current_pred = df.loc[idx, 'Growth_Adj_Pred']
                df.loc[idx, 'Growth_Adj_Pred'] = np.clip(current_pred, lower_bound, upper_bound)
        
        # 四舍五入
        df['Growth_Adj_Pred'] = df['Growth_Adj_Pred'].round().astype(int)
        
        # 删除辅助列
        df = df.drop(columns=['Quarter', 'Low_Season_SD', 'PAX'], errors='ignore')
        
        # 5. 输出到原文件夹
        output_path = os.path.join(output_dir, f"summary_stacked_{self.time_level}.csv")
        df.to_csv(output_path, index=False)
        logging.info(f"调整后的Stacked Summary已保存到 {output_path}")
        
        # 6. 同时输出到Public Data Base文件夹
        public_output_path = os.path.join(self.Public_DIR, f"summary_stacked_{self.time_level}.csv")
        df.to_csv(public_output_path, index=False)
        logging.info(f"调整后的Stacked Summary已保存到 {public_output_path}")
        
        return df

    def plot_predictions(self, stacked_df, agg_level, output_dir):
        os.makedirs(output_dir, exist_ok=True)

        if 'Tour ID' in stacked_df.columns:
            stacked_df['Tour ID'] = stacked_df['Tour ID'].astype(str).str.replace(r'\.0$', '', regex=True)

        for tour_id in self.predict_tour_ids:
            print("-" * 50)
            print(f"DEBUG: 正在尝试为 Tour ID '{tour_id}' (类型: {type(tour_id)}) 绘图")
            print(f"DEBUG: stacked_df['Tour ID'] 的数据类型是: {stacked_df['Tour ID'].dtype}")

            tour_data = stacked_df[stacked_df['Tour ID'] == tour_id]

            if tour_data.empty:
                logging.warning(f"在 stacked_df 中找不到 Tour ID '{tour_id}' 的数据，跳过绘图。")
                similar_ids = [tid for tid in stacked_df['Tour ID'].unique() if tour_id in tid or tid in tour_id]
                if similar_ids:
                    print(f"DEBUG: 在 stacked_df 中找到了相似的 ID: {similar_ids}")
                else:
                    print(f"DEBUG: 在 stacked_df 中未找到任何与 '{tour_id}' 相似的 ID。")
                continue

            dates = tour_data['Date']
            pred_values = tour_data['Pred']
            adj_pred_values = tour_data['Growth_Adj_Pred']
            actual_values = tour_data['Actual']

            plt.figure(figsize=(12, 6))
            plt.plot(dates, pred_values, label='Prediction', marker='x', color='blue', linestyle='--')
            plt.plot(dates, adj_pred_values, label='Growth Adjusted Prediction', marker='^', color='green',
                     linestyle='-.')
            plt.plot(dates, actual_values, label='Actual Pax', marker='o', color='black', linestyle='-')

            plt.legend()
            event_name = self.tour_id_map.get(tour_id, tour_id) if self.tour_id_map else tour_id
            plt.title(f"{tour_id} - {event_name} Predictions ({agg_level.capitalize()})")
            plt.xlabel('Date')
            plt.ylabel('Pax')
            plt.grid(True)

            date_format = '%Y-%m-%d %H:%M' if agg_level == 'hourly' else '%Y-%m-%d'
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(date_format))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.gcf().autofmt_xdate()

            illegal_chars = r'[\<\>\:"/\\|?*]'
            safe_tour_id = re.sub(illegal_chars, '_', str(tour_id))
            safe_event_name = re.sub(illegal_chars, '_', str(event_name))
            filename = f"{safe_tour_id}_{safe_event_name}_{agg_level}_predictions.png"
            filepath = os.path.join(output_dir, filename)

            try:
                plt.savefig(filepath)
                logging.info(f"成功保存图表: {filepath}")
            except Exception as e:
                logging.error(f"保存图表失败 {filepath}: {e}")

            plt.close()

    def run(self):
        valid_agg_levels = self.check_required_files()
        if not valid_agg_levels:
            logging.error("No valid aggregation levels to process.")
            return {'summary': None}

        self.load_failed_tours()
        self.load_tour_id_map()
        self.load_predict_tour_ids()
        self.load_bookings_data()
        agg_folder_map = self.get_agg_folder_map()

        # 只处理指定的时间级别
        agg_level = self.time_level
        folder = agg_folder_map[agg_level]
        output_dir = os.path.join(self.base_dir, "Outputs", folder)

        # Step 1: Load and pivot prediction
        horizontal_prediction = self.load_and_pivot_prediction(agg_level, folder)
        if horizontal_prediction is None:
            return {'summary': None}
        horizontal_prediction_path = os.path.join(output_dir, f"predictions_horizontal_{agg_level}.csv")
        horizontal_prediction.to_csv(horizontal_prediction_path, index=False)
        logging.info(f"Horizontal prediction data saved to {horizontal_prediction_path}")

        # Step 2: Filter successful tours and join with actuals
        successful_prediction = self.filter_successful_tours(horizontal_prediction)
        joined_df = self.join_actual_and_prediction(agg_level, folder, successful_prediction)
        if joined_df is None:
            return {'summary': None}

        # Step 3: Generate summary
        summary_df = self.generate_summary(joined_df, agg_level)
        summary_output_path = os.path.join(output_dir, f"summary_{agg_level}.csv")
        summary_df.to_csv(summary_output_path, index=False)
        logging.info(f"Summary saved to {summary_output_path}")

        # Step 4: Generate forecast
        self.generate_forecast(summary_df, agg_level, output_dir, f"forecast_{agg_level}.csv")

        # Step 5: Generate growth rate adjusted summary
        adjusted_df = self.generate_adjusted_summary(summary_df, agg_level, output_dir)

        # Step 6: Generate adjusted forecast
        self.generate_forecast(adjusted_df, agg_level, output_dir,
                               f"forecast_adjusted_{agg_level}.csv")

        # Step 7: Generate stacked summary
        stacked_df = self.generate_stacked_summary(summary_df, adjusted_df, agg_level, output_dir)

        # Step 8: 调整stacked summary（新增）
        adjusted_stacked_df = self.adjust_stacked_summary(stacked_df, output_dir)

        # Step 9: Plot predictions (使用调整后的数据)
        self.plot_predictions(adjusted_stacked_df, agg_level, output_dir)

        self.all_summary.append(summary_df)
        gc.collect()

        return {'summary': self.all_summary}


if __name__ == "__main__":
    # 示例：处理weekly级别的数据
    processor = PredictionProcessor(
        start_date='2024-01-01', 
        end_date='2025-09-22', 
        sigmoid_k=1,
        sigmoid_max_rate=0.272,
        time_level='weekly'  # 可以改为 'daily', 'monthly', 'hourly'
    )
    processor.run()
