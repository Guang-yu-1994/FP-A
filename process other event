import pandas as pd
import numpy as np
import os
import matplotlib.pyplot as plt
import matplotlib.dates as mdates
import logging
from datetime import datetime
import gc
from dateutil.relativedelta import relativedelta
from aggregate_actuals import ActualsAggregator
import re

BASE_DIR = os.path.dirname(os.path.abspath(__file__))


class PredictionProcessor:
    def __init__(self, start_date='2024-01-01', end_date='2024-12-31', base_dir=BASE_DIR, forecast_start_date=None,
                 sigmoid_k=1, sigmoid_max_rate=0.1886):  # 新增 sigmoid_k 和 sigmoid_max_rate 参数
        self.start_date = pd.to_datetime(start_date)
        self.end_date = pd.to_datetime(end_date)
        if self.start_date > self.end_date:
            raise ValueError("start_date 不能晚于 end_date")

        self.base_dir = base_dir
        # Added Public_DIR as a class attribute
        self.Public_DIR = r"C:\City Experience\Public Data Base"  # Using raw string to handle backslashes

        if forecast_start_date is None:
            self.forecast_start_date = pd.to_datetime(datetime.now() - relativedelta(months=2)).replace(hour=0,
                                                                                                        minute=0,
                                                                                                        second=0,
                                                                                                        microsecond=0)
        else:
            self.forecast_start_date = pd.to_datetime(forecast_start_date)
        self.all_summary = []
        self.failed_tours = set()
        self.tour_id_map = None
        self.predict_tour_ids = set()

        logging.info("正在初始化并运行 ActualsAggregator 以生成实际值 CSV 文件")
        # 将 sigmoid_k 和 sigmoid_max_rate 传递给 ActualsAggregator
        aggregator = ActualsAggregator(base_dir=self.base_dir, start_date=self.start_date, end_date=self.end_date,
                                       sigmoid_k=sigmoid_k, sigmoid_max_rate=sigmoid_max_rate)
        aggregator.process_all_levels()
        logging.info("ActualsAggregator 处理完成，已生成所有时间级别的 actuals CSV 文件")

        # Basic logging configuration for the class. This should ideally be set up once globally.
        # However, to ensure it runs with the instance, it's kept here.
        # Avoids re-configuring if already done by a higher-level script.
        if not logging.getLogger().handlers:  # Check if handlers are already configured
            logging.basicConfig(
                level=logging.INFO,
                format='%(asctime)s - %(levelname)s - %(message)s',
                handlers=[
                    logging.FileHandler(os.path.join(self.base_dir, 'process_other_events.log'), encoding='utf-8'),
                    logging.StreamHandler()
                ]
            )
        logging.info(
            f"初始化 PredictionProcessor，时间范围: {self.start_date} 到 {self.end_date}, 预测开始日期: {self.forecast_start_date}")

    def load_tour_id_map(self):
        """
        Loads the mapping from Tour ID to Event Name.
        Standardizes Tour IDs (lowercase and remove spaces) to match Tour IDs in processed dataframes.
        """
        # Changed file path to use self.Public_DIR
        tour_id_file = os.path.join(self.Public_DIR, "Tour ID.xlsx")

        if os.path.exists(tour_id_file):
            try:
                tour_df = pd.read_excel(tour_id_file, sheet_name="Tour ID")
                if 'Tour ID' in tour_df.columns and 'Event Name' in tour_df.columns:
                    # Key modification: Standardize Tour ID (lowercase and remove spaces)
                    standardized_tour_ids = tour_df['Tour ID'].astype(str).str.lower().str.strip()
                    self.tour_id_map = dict(zip(standardized_tour_ids, tour_df['Event Name'].astype(str)))
                    logging.info(f"Successfully loaded Tour ID to Event Name map from {tour_id_file}")
                else:
                    logging.warning(f"Sheet 'Tour ID' in {tour_id_file} is missing 'Tour ID' or 'Event Name' columns.")
            except Exception as e:
                logging.error(f"Failed to read {tour_id_file}: {str(e)}")
        else:
            logging.warning(f"Tour ID map file not found at {tour_id_file}. Event Names cannot be mapped.")

    def load_predict_tour_ids(self):
        predict_tour_file = os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID to Predict.xlsx")
        if os.path.exists(predict_tour_file):
            try:
                predict_df = pd.read_excel(predict_tour_file)
                if 'Tour ID' in predict_df.columns:
                    self.predict_tour_ids = set(predict_df['Tour ID'].astype(str).str.lower().str.strip())
                    logging.info(f"成功加载 Tour ID to Predict: {self.predict_tour_ids}")
                else:
                    logging.warning(f"{predict_tour_file} 缺少 'Tour ID' 列")
            except Exception as e:
                logging.error(f"读取 {predict_tour_file} 失败: {str(e)}")
        else:
            logging.warning(f"未找到 {predict_tour_file}")

    def get_agg_folder_map(self):
        outputs_dir = os.path.join(self.base_dir, "Outputs")
        if not os.path.exists(outputs_dir):
            logging.error(f"Outputs 文件夹不存在: {outputs_dir}")
            return {}

        possible_levels = ['hourly', 'daily', 'weekly', 'monthly']
        agg_folder_map = {}
        for folder in os.listdir(outputs_dir):
            folder_path = os.path.join(outputs_dir, folder)
            if os.path.isdir(folder_path) and folder.lower() in possible_levels:
                agg_folder_map[folder.lower()] = folder
        return agg_folder_map

    def check_required_files(self):
        agg_folder_map = self.get_agg_folder_map()
        valid_agg_levels = []
        for agg_level, folder in agg_folder_map.items():
            required_files = [
                os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID Failed to Predict.xlsx"),
                os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID to Predict.xlsx"),
                os.path.join(self.base_dir, "Outputs", folder, f"predictions_{agg_level}.xlsx"),
                os.path.join(self.base_dir, "Outputs", folder, f"actuals_{agg_level}_horizontal.csv")
            ]
            if all(os.path.exists(file) for file in required_files):
                valid_agg_levels.append(agg_level)
        return valid_agg_levels

    def load_failed_tours(self):
        failed_tours_file = os.path.join(self.base_dir, "Spreadsheets Source", "Tour ID Failed to Predict.xlsx")
        if os.path.exists(failed_tours_file):
            try:
                failed_df = pd.read_excel(failed_tours_file)
                if 'Tour ID' in failed_df.columns:
                    self.failed_tours = set(str(tour).strip().lower() for tour in failed_df['Tour ID'].unique())
                    logging.info(f"检测到建模失败 Tour ID: {self.failed_tours}")
            except Exception as e:
                logging.error(f"读取 {failed_tours_file} 失败: {str(e)}")

    def load_and_pivot_prediction(self, agg_level, folder):
        prediction_file = os.path.join(self.base_dir, "Outputs", folder, f"predictions_{agg_level}.xlsx")
        if not os.path.exists(prediction_file):
            logging.error(f"未找到 {prediction_file}")
            return None

        prediction_df = pd.read_excel(prediction_file, sheet_name=f"{agg_level}_predictions")
        if not {'Date', 'Tour ID', 'Ensemble Prediction'}.issubset(prediction_df.columns):
            logging.error(f"{prediction_file} 缺少必要列")
            return None

        prediction_df['Date'] = pd.to_datetime(prediction_df['Date'], errors='coerce')
        horizontal_prediction = prediction_df.pivot(index='Tour ID', columns='Date', values='Ensemble Prediction')
        horizontal_prediction = horizontal_prediction.reset_index()
        date_format = '%Y-%m-%d %H:%M:%S' if agg_level == 'hourly' else '%Y-%m-%d'
        horizontal_prediction.columns = ['Tour ID'] + [f"{col.strftime(date_format)}_Pred" for col in
                                                       horizontal_prediction.columns[1:]]
        horizontal_prediction['Tour ID'] = horizontal_prediction['Tour ID'].astype(str).str.lower().str.strip()
        return horizontal_prediction

    def filter_successful_tours(self, horizontal_prediction):
        return horizontal_prediction[~horizontal_prediction['Tour ID'].isin(self.failed_tours)]

    def join_actual_and_prediction(self, agg_level, folder, successful_prediction):
        actuals_file = os.path.join(self.base_dir, "Outputs", folder, f"actuals_{agg_level}_horizontal.csv")
        actuals_df = pd.read_csv(actuals_file)
        actuals_df['Tour ID'] = actuals_df['Tour ID'].astype(str).str.lower().str.strip()

        total_pred_row = successful_prediction[successful_prediction['Tour ID'] == 'total']
        if total_pred_row.empty:
            logging.error("未找到 Tour ID = 'total' 的预测数据")
            return None

        joined_df = actuals_df.merge(successful_prediction, on='Tour ID', how='left', suffixes=('_Actual', '_Pred'))
        for col in joined_df.columns:
            if col.endswith('_Pred'):
                failed_mask = joined_df[col].isna()
                total_pred_value = total_pred_row[col].values[0] if not total_pred_row[col].isna().all() else 0
                joined_df.loc[failed_mask, col] = (
                        total_pred_value * joined_df.loc[failed_mask, 'Pax Ratio']).round().astype(int)

        return joined_df

    def generate_summary(self, joined_df, agg_level):
        if self.tour_id_map is None:
            self.load_tour_id_map()

        # Copy DataFrame to avoid fragmentation
        summary_df = joined_df.copy()
        # Add temporary debugging code: Find rows where Event Name is still the Tour ID itself
        # This means these Tour IDs did not find a match in tour_id_map
        summary_df['Event Name'] = summary_df['Tour ID'].map(self.tour_id_map).fillna(summary_df['Tour ID'])
        unmapped_tours = summary_df[summary_df['Event Name'] == summary_df['Tour ID']]
        if not unmapped_tours.empty:
            logging.warning(
                f"The following Tour IDs could not be mapped to an Event Name and were filled with their own Tour ID:\n{unmapped_tours[['Tour ID', 'Event Name']].to_string()}")

        # Convert _Pred and _Actual columns to integers
        date_cols_pred = [col for col in summary_df.columns if col.endswith('_Pred')]
        date_cols_actual = [col for col in summary_df.columns if col.endswith('_Actual')]
        for col in date_cols_pred + date_cols_actual:
            summary_df[col] = summary_df[col].fillna(0).astype(int)

        # Use pd.concat to merge Is_Predict_Tour column
        is_predict_tour = pd.Series(
            summary_df['Tour ID'].isin(self.predict_tour_ids).astype(int),
            index=summary_df.index,
            name='Is_Predict_Tour'
        )
        summary_df = pd.concat([summary_df, is_predict_tour], axis=1)

        # Sort by Is_Predict_Tour and Period Sum in descending order
        summary_df = summary_df.sort_values(by=['Is_Predict_Tour', 'Period Sum'], ascending=[False, False])
        summary_df = summary_df.drop(columns=['Is_Predict_Tour'])

        return summary_df[['Tour ID', 'Event Name'] + date_cols_pred + date_cols_actual +
                          ['YTD_This_Year', 'YTD_Last_Year', 'Growth_Rate', 'Sigmoid_Growth_Rate', 'Period Sum',
                           'Pax Ratio']]

    def generate_forecast(self, source_df, agg_level, output_dir, output_filename):
        date_cols_pred = [col for col in source_df.columns if col.endswith('_Pred')]
        forecast_cols = ['Tour ID', 'Event Name'] + date_cols_pred
        forecast_df = source_df[forecast_cols].copy()

        # Remove _Pred suffix, rename columns to dates
        forecast_df.columns = ['Tour ID', 'Event Name'] + [col.replace('_Pred', '') for col in date_cols_pred]

        # Filter date columns starting from forecast_start_date
        date_columns = [col for col in forecast_df.columns if col not in ['Tour ID', 'Event Name']]
        valid_dates = [col for col in date_columns if pd.to_datetime(col) >= self.forecast_start_date]
        forecast_df = forecast_df[['Tour ID', 'Event Name'] + valid_dates]

        # Save forecast CSV
        forecast_output_path = os.path.join(output_dir, output_filename)
        forecast_df.to_csv(forecast_output_path, index=False)
        logging.info(f"Forecast saved to {forecast_output_path}")
        return forecast_df

    def generate_adjusted_summary(self, summary_df, agg_level, output_dir):
        adjusted_df = summary_df.copy()
        date_cols_pred = [col for col in summary_df.columns if col.endswith('_Pred')]

        for col in date_cols_pred:
            adjusted_df[col] = (adjusted_df[col] * (1 + adjusted_df['Sigmoid_Growth_Rate'].fillna(0))).round().astype(
                int)

        adjusted_output_path = os.path.join(output_dir, f"summary_adjusted_{agg_level}.csv")
        adjusted_df.to_csv(adjusted_output_path, index=False)
        logging.info(f"Growth Rate Adjusted Summary (using Sigmoid_Growth_Rate) saved to {adjusted_output_path}")
        return adjusted_df

    def generate_stacked_summary(self, summary_df, adjusted_df, agg_level, output_dir):
        date_cols_pred = [col for col in summary_df.columns if col.endswith('_Pred')]
        date_cols_actual = [col for col in summary_df.columns if col.endswith('_Actual')]

        # Stack Pred and Actual of summary
        pred_melt = pd.melt(
            summary_df,
            id_vars=['Tour ID', 'Event Name'],
            value_vars=date_cols_pred,
            var_name='Date',
            value_name='Pred'
        )
        pred_melt['Date'] = pred_melt['Date'].str.replace('_Pred', '')

        actual_melt = pd.melt(
            summary_df,
            id_vars=['Tour ID', 'Event Name'],
            value_vars=date_cols_actual,
            var_name='Date',
            value_name='Actual'
        )
        actual_melt['Date'] = actual_melt['Date'].str.replace('_Actual', '')

        # Stack Pred of adjusted summary
        adj_pred_melt = pd.melt(
            adjusted_df,
            id_vars=['Tour ID', 'Event Name'],
            value_vars=date_cols_pred,
            var_name='Date',
            value_name='Growth_Adj_Pred'
        )
        adj_pred_melt['Date'] = adj_pred_melt['Date'].str.replace('_Pred', '')

        # Merge the three melted DataFrames
        stacked_df = pred_melt.merge(actual_melt, on=['Tour ID', 'Event Name', 'Date'], how='outer')
        stacked_df = stacked_df.merge(adj_pred_melt, on=['Tour ID', 'Event Name', 'Date'], how='outer')
        stacked_df['Date'] = pd.to_datetime(stacked_df['Date'])

        # Save stacked summary
        stacked_output_path = os.path.join(output_dir, f"summary_stacked_{agg_level}.csv")
        stacked_df.to_csv(stacked_output_path, index=False)
        logging.info(f"Stacked Summary saved to {stacked_output_path}")
        return stacked_df

    def plot_predictions(self, stacked_df, agg_level, output_dir):
        os.makedirs(output_dir, exist_ok=True)

        # --- 新增的稳健性代码 ---
        # 强制将 stacked_df 中的 'Tour ID' 列转换为字符串，并去除 .0 后缀
        # 以处理 12345.0 这样的情况
        if 'Tour ID' in stacked_df.columns:
            stacked_df['Tour ID'] = stacked_df['Tour ID'].astype(str).str.replace(r'\.0$', '', regex=True)
        # --- 稳健性代码结束 ---

        for tour_id in self.predict_tour_ids:
            # --- 新增的调试代码 ---
            print("-" * 50)
            print(f"DEBUG: 正在尝试为 Tour ID '{tour_id}' (类型: {type(tour_id)}) 绘图")
            print(f"DEBUG: stacked_df['Tour ID'] 的数据类型是: {stacked_df['Tour ID'].dtype}")
            # --- 调试代码结束 ---

            tour_data = stacked_df[stacked_df['Tour ID'] == tour_id]

            if tour_data.empty:
                logging.warning(f"在 stacked_df 中找不到 Tour ID '{tour_id}' 的数据，跳过绘图。")
                # --- 新增的调试代码 ---
                # 如果找不到，检查是否存在看起来相似的ID
                similar_ids = [tid for tid in stacked_df['Tour ID'].unique() if tour_id in tid or tid in tour_id]
                if similar_ids:
                    print(f"DEBUG: 在 stacked_df 中找到了相似的 ID: {similar_ids}")
                else:
                    print(f"DEBUG: 在 stacked_df 中未找到任何与 '{tour_id}' 相似的 ID。")
                # --- 调试代码结束 ---
                continue

            dates = tour_data['Date']
            pred_values = tour_data['Pred']
            adj_pred_values = tour_data['Growth_Adj_Pred']
            actual_values = tour_data['Actual']

            plt.figure(figsize=(12, 6))
            plt.plot(dates, pred_values, label='Prediction', marker='x', color='blue', linestyle='--')
            plt.plot(dates, adj_pred_values, label='Growth Adjusted Prediction', marker='^', color='green',
                     linestyle='-.')
            plt.plot(dates, actual_values, label='Actual Pax', marker='o', color='black', linestyle='-')

            plt.legend()
            event_name = self.tour_id_map.get(tour_id, tour_id) if self.tour_id_map else tour_id
            plt.title(f"{tour_id} - {event_name} Predictions ({agg_level.capitalize()})")
            plt.xlabel('Date')
            plt.ylabel('Pax')
            plt.grid(True)

            date_format = '%Y-%m-%d %H:%M' if agg_level == 'hourly' else '%Y-%m-%d'
            plt.gca().xaxis.set_major_formatter(mdates.DateFormatter(date_format))
            plt.gca().xaxis.set_major_locator(mdates.AutoDateLocator())
            plt.gcf().autofmt_xdate()

            # --- 修改后的文件名净化代码 ---
            # 定义在Windows文件名中非法的字符集
            illegal_chars = r'[\<\>\:"/\\|?*]'

            # 净化 tour_id 和 event_name
            safe_tour_id = re.sub(illegal_chars, '_', str(tour_id))
            safe_event_name = re.sub(illegal_chars, '_', str(event_name))

            # 构建安全的文件名和完整路径
            filename = f"{safe_tour_id}_{safe_event_name}_{agg_level}_predictions.png"
            filepath = os.path.join(output_dir, filename)

            # 使用 try-except 块来增加保存文件操作的稳健性
            try:
                plt.savefig(filepath)
                logging.info(f"成功保存图表: {filepath}")
            except Exception as e:
                logging.error(f"保存图表失败 {filepath}: {e}")

            plt.close()
            # --- 修改结束 ---

    def run(self):
        valid_agg_levels = self.check_required_files()
        if not valid_agg_levels:
            logging.error("No valid aggregation levels to process.")
            return {'summary': None}

        self.load_failed_tours()
        self.load_tour_id_map()
        self.load_predict_tour_ids()
        agg_folder_map = self.get_agg_folder_map()

        for agg_level in valid_agg_levels:
            folder = agg_folder_map[agg_level]
            output_dir = os.path.join(self.base_dir, "Outputs", folder)

            # Step 1: Load and pivot prediction
            horizontal_prediction = self.load_and_pivot_prediction(agg_level, folder)
            if horizontal_prediction is None:
                continue
            horizontal_prediction_path = os.path.join(output_dir, f"predictions_horizontal_{agg_level}.csv")
            horizontal_prediction.to_csv(horizontal_prediction_path, index=False)
            logging.info(f"Horizontal prediction data saved to {horizontal_prediction_path}")

            # Step 2: Filter successful tours and join with actuals
            successful_prediction = self.filter_successful_tours(horizontal_prediction)
            joined_df = self.join_actual_and_prediction(agg_level, folder, successful_prediction)
            if joined_df is None:
                continue

            # Step 3: Generate summary
            summary_df = self.generate_summary(joined_df, agg_level)
            summary_output_path = os.path.join(output_dir, f"summary_{agg_level}.csv")
            summary_df.to_csv(summary_output_path, index=False)
            logging.info(f"Summary saved to {summary_output_path}")

            # Step 4: Generate forecast
            self.generate_forecast(summary_df, agg_level, output_dir, f"forecast_{agg_level}.csv")

            # Step 5: Generate growth rate adjusted summary
            adjusted_df = self.generate_adjusted_summary(summary_df, agg_level, output_dir)

            # Step 6: Generate adjusted forecast
            self.generate_forecast(adjusted_df, agg_level, output_dir,
                                   f"forecast_adjusted_{agg_level}.csv")

            # Step 7: Generate stacked summary
            stacked_df = self.generate_stacked_summary(summary_df, adjusted_df, agg_level, output_dir)

            # Step 8: Plot predictions
            self.plot_predictions(stacked_df, agg_level, output_dir)

            self.all_summary.append(summary_df)
            gc.collect()

        return {'summary': self.all_summary}


if __name__ == "__main__":
    # You can pass sigmoid_k and sigmoid_max_rate here
    processor = PredictionProcessor(start_date='2024-01-01', end_date='2025-09-22', sigmoid_k=1,
                                    sigmoid_max_rate=0.272)
    processor.run()
